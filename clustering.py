# -*- coding: utf-8 -*-
"""ML_clustering_task_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GH2vN9DgpPITJ4pjU6gsuvVbQDlCOsjK

#https://archive.ics.uci.edu/dataset/837/product+classification+and+clustering
#Dataset link
"""

# import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# import the dataset
file_path = 'pricerunner_aggregate.csv'
dataset = pd.read_csv(file_path)

dataset.head()

dataset.tail()

print(dataset.columns)

dataset.info()

dataset.describe()

"""Here choose 3000 rows from the 35311"""

# Sample 3,000 rows from the dataset
data = dataset.sample(n=3000, random_state=42)

# Display basic information
print("Dataset Shape:", data.shape)

data.info()

data.head()

data.describe()

# Display data types
print("Data Types:\n", data.dtypes)

print("\nMissing Values:\n", data.isnull().sum())

# Display unique values in each categorical column
for column in data.select_dtypes(include='object').columns:
    print(f"\nUnique values in '{column}':", data[column].nunique())

sns.pairplot(data.iloc[:,[2,3,5]])

# Plot distributions of numerical features
numerical_columns = data.select_dtypes(include=['int64', 'float64']).columns
for column in numerical_columns:
    plt.figure(figsize=(4, 2))
    sns.histplot(data[column], kde=True)
    plt.title(f'Distribution of {column}')
    plt.show()

# Plot bar charts for categorical features
categorical_columns = data.select_dtypes(include='object').columns
for column in categorical_columns:
    plt.figure(figsize=(6, 3))
    data[column].value_counts().head(10).plot(kind='bar')
    plt.title(f'Value counts of {column} (Top 10)')
    plt.xlabel(column)
    plt.ylabel('Count')
    plt.show()

# Plot correlation heatmap for numerical features
plt.figure(figsize=(4, 2))
sns.heatmap(data[numerical_columns].corr(), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap of Numerical Features")
plt.show()

from sklearn.cluster import KMeans

from sklearn.preprocessing import StandardScaler, LabelEncoder

#  Encoding Categorical Features
# Label Encoding for simplicity
label_encoders = {}
for column in categorical_columns:
    le = LabelEncoder()
    data[column] = le.fit_transform(data[column])
    label_encoders[column] = le

#  Scaling Numerical Features
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Display processed data sample
data.head()



# from sklearn.metrics import calinski_harabasz_score
# ch_score = calinski_harabasz_score(data, kmeans_labels)
# print("Calinski-Harabasz Index:", ch_score)

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Calculate WCSS for different numbers of clusters
wcss = []
for i in range(1, 11):
    kmeans = KMeans(n_clusters=i, init='k-means++', random_state=42)
    kmeans.fit(data)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.figure(figsize=(6, 3))
plt.plot(range(1, 11), wcss, marker='o', linestyle='-')
plt.xlabel('Number of Clusters')
plt.ylabel('WCSS')
plt.title('Elbow Method for Optimal Clusters')
plt.show()

from sklearn.cluster import AgglomerativeClustering

# Applying Hierarchical Clustering
hierarchical = AgglomerativeClustering(n_clusters=3)
hierarchical_labels = hierarchical.fit_predict(data)

# Calculate the silhouette score for Hierarchical Clustering
hierarchical_silhouette = silhouette_score(data, hierarchical_labels)
print("Hierarchical Clustering Silhouette Score:", hierarchical_silhouette)

# Add Hierarchical cluster labels to the data for visualization
data['Hierarchical_Cluster'] = hierarchical_labels

from scipy.cluster.hierarchy import dendrogram, linkage

# Generate linkage matrix
linked = linkage(data, method='ward')

# Plot the dendrogram
plt.figure(figsize=(6, 3))
dendrogram(linked, truncate_mode='lastp', p=12, show_leaf_counts=True, leaf_rotation=45, leaf_font_size=15)
plt.title('Dendrogram for Hierarchical Clustering')
plt.xlabel('Sample Index or Cluster Size')
plt.ylabel('Distance')
plt.show()

# from sklearn.decomposition import PCA

# # Reduce data to 2D for visualization
# pca = PCA(n_components=2)
# reduced_data = pca.fit_transform(data)

# # Visualize K-Means clusters
# plt.figure(figsize=(8, 4))
# plt.subplot(1, 2, 1)
# sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=kmeans_labels, palette='viridis')
# plt.title("K-Means Clustering")

# # Visualize Hierarchical clusters
# plt.subplot(1, 2, 2)
# sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=hierarchical_labels, palette='viridis')
# plt.title("Hierarchical Clustering")
# plt.show()

# Compare silhouette scores
print("K-Means Silhouette Score:", kmeans_silhouette)
print("Hierarchical Clustering Silhouette Score:", hierarchical_silhouette)

from sklearn.decomposition import PCA

# Reduce data to 2D for visualization
pca = PCA(n_components=2)
pca_data = pca.fit_transform(data)

# Plot the PCA results with K-Means and Hierarchical Clusters
plt.figure(figsize=(8, 4))

# K-Means Clustering Visualization
plt.subplot(1, 2, 1)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=kmeans_labels, cmap='viridis')
plt.title("K-Means Clustering with PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

# Hierarchical Clustering Visualization
plt.subplot(1, 2, 2)
plt.scatter(pca_data[:, 0], pca_data[:, 1], c=hierarchical_labels, cmap='viridis')
plt.title("Hierarchical Clustering with PCA")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")

plt.tight_layout()
plt.show()

from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns

# Reduce data to 3D for visualization
pca = PCA(n_components=3)
reduced_data_3d = pca.fit_transform(data)

# Set up the 3D figure
fig = plt.figure(figsize=(14, 6))

# Visualize K-Means clusters in 3D
ax1 = fig.add_subplot(121, projection='3d')
scatter = ax1.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2],
                      c=kmeans_labels, cmap='viridis', marker='o')
ax1.set_title("K-Means Clustering")
ax1.set_xlabel("PCA Component 1")
ax1.set_ylabel("PCA Component 2")
ax1.set_zlabel("PCA Component 3")
fig.colorbar(scatter, ax=ax1, label="Cluster")

# Visualize Hierarchical clusters in 3D
ax2 = fig.add_subplot(122, projection='3d')
scatter = ax2.scatter(reduced_data_3d[:, 0], reduced_data_3d[:, 1], reduced_data_3d[:, 2],
                      c=hierarchical_labels, cmap='viridis', marker='o')
ax2.set_title("Hierarchical Clustering")
ax2.set_xlabel("PCA Component 1")
ax2.set_ylabel("PCA Component 2")
ax2.set_zlabel("PCA Component 3")
fig.colorbar(scatter, ax=ax2, label="Cluster")

plt.tight_layout()
plt.show()

from sklearn.manifold import TSNE


tsne = TSNE(n_components=2, random_state=42)
reduced_data = tsne.fit_transform(data)

plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=kmeans_labels, cmap='plasma', s=50)
plt.title("K-means Clustering Visualization")
plt.show()

tsne = TSNE(n_components=2, random_state=42)
reduced_data = tsne.fit_transform(data)


plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=hierarchical_labels, cmap='plasma', s=50)
plt.title("Hierarchical Clustering Visualization")
plt.show()

from sklearn.metrics import calinski_harabasz_score

# Calculate Calinski-Harabasz Index for K-Means
kmeans_ch_score = calinski_harabasz_score(data, kmeans_labels)
print("Calinski-Harabasz Index for K-Means:", kmeans_ch_score)

# Calculate for Agglomerative Clustering
agglo_ch_score = calinski_harabasz_score(data, hierarchical_labels)
print("Calinski-Harabasz Index for Agglomerative Clustering:", agglo_ch_score)

from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import seaborn as sns

# Applying K-Means Clustering
kmeans = KMeans(n_clusters=3, random_state=42)
kmeans_labels = kmeans.fit_predict(data)

# Get the K-Means centroids
kmeans_centroids = kmeans.cluster_centers_

# Reduce data and centroids to 2D for visualization
pca = PCA(n_components=2)
reduced_data = pca.fit_transform(data)
reduced_centroids = pca.transform(kmeans_centroids)

# Plotting the clusters with centroids
plt.figure(figsize=(8, 4))

# Visualize K-Means clusters with centroids
plt.subplot(1, 2, 1)
sns.scatterplot(x=reduced_data[:, 0], y=reduced_data[:, 1], hue=kmeans_labels, palette='viridis', s=50, alpha=0.7)
plt.scatter(reduced_centroids[:, 0], reduced_centroids[:, 1], color='blue', marker='o', s=100, label="Centroids")
plt.title("K-Means Clustering with Centroids")
plt.xlabel("PCA Component 1")
plt.ylabel("PCA Component 2")
plt.legend()

plt.tight_layout()
plt.show()

"""For the **Business Insights and Recommendations** section, we’ll focus on interpreting the clustering results and suggesting ways the organization could benefit from these insights.

### 6. **Business Insights and Interpretation**

After analyzing the clusters generated by K-Means and Hierarchical Clustering, here’s how the organization might use these results:

1. **Understanding Customer Segments**:
   - The clusters can help identify distinct customer or product segments, like high-demand products, frequently discounted items, or high-margin products.
   - By identifying which products or services fall into each cluster, the business can tailor its marketing and sales strategies to match the preferences of each segment.

2. **Inventory Management**:
   - The clusters can reveal patterns in products that are frequently purchased together or have similar demand cycles. For example, if certain products are clustered due to similar purchasing patterns, the business could stock these products accordingly, reducing overstock or stockouts.
   
3. **Pricing Strategy Optimization**:
   - Clustering can highlight groups of products that may benefit from price adjustments. For instance, one cluster might include premium products with less sensitivity to price changes, allowing for a higher margin strategy.
   - Another cluster may include budget-friendly products that are price-sensitive, suggesting a volume-based approach.

4. **Marketing and Personalization**:
   - Each cluster can represent a specific type of customer preference. The business can create targeted promotions based on cluster characteristics, like offering bundled deals for products in the same cluster.
   - For online platforms, clustering insights can enhance recommendation systems, showing customers products similar to their previous purchases.

### 7. **Actionable Recommendations**

Based on these insights, here are some specific recommendations:

1. **Targeted Marketing Campaigns**:
   - Design campaigns tailored to each cluster. For example, promote high-margin products to premium customers identified in one cluster, while offering discounts for more price-sensitive segments.

2. **Improved Inventory Planning**:
   - Use cluster information to optimize stock levels, ensuring popular items are always in stock, while reducing inventory for slower-moving items.
   - Group similar products together in warehouses or retail spaces, simplifying logistics and improving customer experience.

3. **Enhanced Product Recommendations**:
   - If applied to an e-commerce platform, implement a recommendation system based on clustering, suggesting items from the same cluster that align with customer preferences.

4. **Price Adjustments**:
   - For clusters indicating high-value products, explore price increases to maximize revenue.
   - For clusters with competitive or budget products, implement price cuts to drive volume.


"""

x = data.iloc[:,1:9]
sns.pairplot(x)

